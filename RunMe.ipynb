{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RunMe.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Ay-w1z7FVss_","colab_type":"text"},"cell_type":"markdown","source":["# Installation"]},{"metadata":{"id":"7FpKAgXTvKUD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":4}],"base_uri":"https://localhost:8080/","height":105},"cellView":"code","outputId":"4e3c4419-e2e9-4221-837d-4a048a49b796","executionInfo":{"status":"ok","timestamp":1522134131836,"user_tz":-120,"elapsed":30352,"user":{"displayName":"jean luc laos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102781628766905780190"}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"wof77EHeFe4J","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":5},{"item_id":6}],"base_uri":"https://localhost:8080/","height":351},"outputId":"08102864-80f3-47fd-d686-a906f16062bb","executionInfo":{"status":"error","timestamp":1522134143580,"user_tz":-120,"elapsed":11704,"user":{"displayName":"jean luc laos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102781628766905780190"}}},"cell_type":"code","source":["import os, sys\n","from os.path import isfile, join\n","import numpy as np\n","from PIL import Image\n","\n","os.chdir(\"/content/\")\n","!ls\n","!mkdir -p drive\n","!google-drive-ocamlfuse drive\n","!pip install -q keras\n","!pip install -q opencv-python\n","!apt install -y libsm6 libxext6\n","!pip install -q keras\n","\n","os.chdir(\"/content/drive/Colab/Portable/\")\n","# print \"Back to current dir:\"\n","# !ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["datalab  drive\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libsm6 is already the newest version (2:1.2.2-1).\n","libxext6 is already the newest version (2:1.3.3-1).\n","0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mOSError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-3-0479fbc99b97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'pip install -q keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/Colab/Portable/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# print \"Back to current dir:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# !ls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Colab/Portable/'"]}]},{"metadata":{"id":"Wlsog-5qeP7_","colab_type":"text"},"cell_type":"markdown","source":["#1. Prepare data"]},{"metadata":{"id":"VSCUJ9bikVNo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["PATH_INPUT = \"input/\"\n","PATH_OUTPUT = \"output/\"\n","\n","original_imgs_test = PATH_INPUT\n","\n","model_path = \"model/weights.hdf5\"\n","architechture_path = \"model/architecture.json\"\n","\n","channels = 3\n","# height = 1860\n","# width = 1328\n","\n","height = 2336\n","width = 1696\n","\n","\n","# dimension of the patches\n","patch_height = 32\n","patch_width = 32\n","\n","#the stride in case output with average\n","stride_height = 20\n","stride_width = 20\n","assert (stride_height < patch_height and stride_width < patch_width)\n","\n","# PATH_TEMP = \"temp/\"\n","# temp_file = \"data.hdf5\"\n","# dataset_path = PATH_TEMP"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8Rv4hDZyeS8n","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"cellView":"code","outputId":"ecdaeaae-98b9-43c4-d18c-476e49c06136","executionInfo":{"status":"ok","timestamp":1522130974243,"user_tz":-420,"elapsed":30637,"user":{"displayName":"Tai Cola","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"115929984205874480471"}}},"cell_type":"code","source":["import h5py\n","import numpy as np\n","from PIL import Image\n","\n","from os import listdir\n","from os.path import isfile, join\n","\n","#Python\n","import numpy as np\n","# import ConfigParser\n","\n","#Keras\n","from keras.models import model_from_json\n","from keras.models import Model\n","\n","import sys\n","sys.path.insert(0, './lib/')\n","# help_functions.py\n","from help_functions import load_hdf5, write_hdf5, rgb2gray, group_images, visualize, masks_Unet, pred_to_imgs\n","\n","# extract_patches.py\n","from extract_patches import recompone\n","from extract_patches import recompone_overlap\n","from extract_patches import paint_border\n","from extract_patches import paint_border_overlap\n","from extract_patches import extract_ordered_overlap\n","from extract_patches import kill_border\n","from extract_patches import pred_not_only_FOV\n","from extract_patches import get_data_testing\n","from extract_patches import get_data_testing_overlap\n","\n","# pre_processing.py\n","from pre_processing import my_PreProc\n","\n","def write_hdf5(arr,outfile):\n","    with h5py.File(outfile,\"w\") as f:\n","        f.create_dataset(\"image\", data=arr, dtype=arr.dtype)\n","\n","def get_datasets(imgs_dir):\n","    \n","    Nimgs = len([f for f in listdir(imgs_dir) if isfile(join(imgs_dir, f))])\n","    imgs = np.empty((Nimgs,height,width,channels))\n","    for path, subdirs, files in os.walk(imgs_dir): #list all files, directories in the path\n","        for i in range(len(files)):\n","            print (\"image: \" +files[i])\n","            \n","            ### write some lines of code to:\n","            ### - check if image's size is different to requirement, then resize and save to output folder\n","            ### - else: make a copy to output folder\n","            \n","            img = Image.open(imgs_dir+files[i])\n","            imgs[i] = np.asarray(img)    \n","    imgs = np.transpose(imgs,(0,3,1,2))\n","    assert(imgs.shape == (Nimgs,channels,height,width))\n","    return imgs, files\n","\n","def get_data(imgs_test, patch_height, patch_width, stride_height, stride_width):\n","    test_imgs_original = imgs_test\n","    test_imgs = my_PreProc(test_imgs_original)\n","    test_imgs = test_imgs[:,:,:,:]\n","    test_imgs = paint_border_overlap(test_imgs, patch_height, patch_width, stride_height, stride_width)\n","\n","#     print \"\\ntest images shape:\"\n","#     print test_imgs.shape\n","#     print \"range (min-max): \" +str(np.min(test_imgs)) +' - '+str(np.max(test_imgs))\n","   \n","    #extract the TEST patches from the full images\n","    patches_imgs_test = extract_ordered_overlap(test_imgs,patch_height,patch_width,stride_height,stride_width)\n","\n","#     print \"\\ntest PATCHES images shape:\"\n","#     print patches_imgs_test.shape\n","#     print \"range (min-max): \" +str(np.min(patches_imgs_test)) +' - '+str(np.max(patches_imgs_test))\n","    \n","    return patches_imgs_test, test_imgs.shape[2], test_imgs.shape[3]\n","  \n","def createDir(directory):\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","    else:\n","        print directory + \" already exists\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"Q5oRq6OrfEt5","colab_type":"text"},"cell_type":"markdown","source":["#2. Predictions"]},{"metadata":{"id":"N92UMNV_fWSg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":16}],"base_uri":"https://localhost:8080/","height":544},"outputId":"c0e01aab-04ad-40ee-bfb7-15d9c205e597","executionInfo":{"status":"ok","timestamp":1522131076813,"user_tz":-420,"elapsed":99421,"user":{"displayName":"Tai Cola","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"115929984205874480471"}}},"cell_type":"code","source":["#load testing datasets\n","imgs_test, file_names = get_datasets(original_imgs_test)\n","print (\"test datasets loaded\")\n","print(imgs_test.shape)\n","# write_hdf5(imgs_test,dataset_path + temp_file)\n","\n","#original test images\n","test_imgs_orig = imgs_test\n","full_img_height = test_imgs_orig.shape[2]\n","full_img_width = test_imgs_orig.shape[3]\n","\n","#Images to patches:\n","patches_imgs_test = None\n","new_height = None\n","new_width = None\n","masks_test  = None\n","patches_masks_test = None\n","# average_mode = True\n","# if average_mode == True:\n","patches_imgs_test, new_height, new_width = get_data(\n","    imgs_test = imgs_test,\n","    patch_height = patch_height,\n","    patch_width = patch_width,\n","    stride_height = stride_height,\n","    stride_width = stride_width\n",")\n","\n","#Load model\n","model = model_from_json(open(architechture_path).read())\n","model.load_weights(model_path)\n","\n","#Calculate the predictions\n","predictions = model.predict(patches_imgs_test, batch_size=32, verbose=2)\n","\n","# print \"predicted images size :\"\n","# print predictions.shape\n","\n","#===== Convert the prediction arrays in corresponding images\n","pred_patches = pred_to_imgs(predictions, patch_height, patch_width, \"original\")\n","\n","pred_imgs = None\n","\n","# average_mode = True\n","# if average_mode == True:\n","pred_imgs = recompone_overlap(pred_patches, new_height, new_width, stride_height, stride_width)# predictions\n","# else:\n","#     pred_imgs = recompone(pred_patches,13,12)\n","\n","\n","pred_imgs = pred_imgs[:,:,0:full_img_height,0:full_img_width]\n","# print \"pred imgs shape: \" +str(pred_imgs.shape)\n","\n","assert(len(file_names) == pred_imgs.shape[0])\n","\n","N_predicted = pred_imgs.shape[0]\n","# group = 1\n","\n","# Save predictions to files\n","for i in range(int(N_predicted)):\n","    pred_stripe = group_images(pred_imgs[i:i+1,:,:,:],1)\n","    file_name =  file_names[i]\n","    visualize(pred_stripe, \"output/\" + file_name[0:len(file_name)-4] + \"_pred\")\n","\n","print \"All done! Please check the output folder\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["image: 26_4_12_TW10001 (c31071e6).jpg\n","image: 26_4_12_TW40001 (fae95c4d).jpg\n","image: 26_5_12_TB1_00_300001 (77b69c52).jpg\n","image: 26_5_12_TB1_30_900001 (5093daa9).jpg\n","image: 26_5_12_TB1_60_900001 (88abbddd).jpg\n","image: 26_5_12_TB2_00_300001 (94f4a4a0).jpg\n","image: 26_5_12_TB2_30_600001 (0be70fee).jpg\n","image: 26_5_12_TB2_60_900001 (0418a72b).jpg\n","image: 26_5_12_TB3_00_300001 (e590f112).jpg\n","test datasets loaded\n","(9, 3, 2336, 1696)\n","\n","the side H is not compatible with the selected stride of 20\n","img_h 2336, patch_h 32, stride_h 20\n","(img_h - patch_h) MOD stride_h: 4\n","So the H dim will be padded with additional 16 pixels\n","the side W is not compatible with the selected stride of 20\n","img_w 1696, patch_w 32, stride_w 20\n","(img_w - patch_w) MOD stride_w: 4\n","So the W dim will be padded with additional 16 pixels\n","new full images shape: \n","(9, 3, 2352, 1712)\n","Number of patches on h : 117\n","Number of patches on w : 85\n","number of patches per image: 9945, totally for this dataset: 89505\n","N_patches_h: 117\n","N_patches_w: 85\n","N_patches_img: 9945\n","According to the dimension inserted, there are 9 full images (of 2352x1712 each)\n","(9, 1, 2352, 1712)\n","All done! Please check the output folder\n"],"name":"stdout"}]}]}